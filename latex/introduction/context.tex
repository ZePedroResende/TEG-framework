%*** MELHORAR ESTE TEXTO DE INTRODUCAO. MAIS TEXTO***

%In modern science, the need to analyze and process large amounts of data and information has been steadily but surely growing. This need has come to play a significant role to get relevant results from this data-sets.
%\par These analyses are mostly done by computer applications that take data-sets and process raw data into useful information with scientific value allowing scientists in all the major fields to achieve discoveries with ease.



Large computing platforms are integrating larger amounts of computing servers to further increase their computational resources.
Current computing servers may include multiple multicore and/or manycore devices, each with different micro-architectures that may impact the efficiency of an executing application.
To adequately use the resources in these large platforms it is first required to use the resources in each server as efficiently as possible, which requires deep knowledge of their characteristics to tune the performance of the code to execute.

Each new multicore device, also known as CPU, packs more computing cores than ever before.
For instance, the latest server AMD multicore device has up to 64 cores with 2-way SMT, which can be used in a multi-socket server for up to 128 cores in a single system.
The most popular manycore device, the Intel Xeon Phi, has up to 72 cores per chip, each with 4-way SMT.
Current multicore and manycore devices rely more and more on SIMD extensions, which has the potential to accelerate specific workloads through the execution of a single operation on multiple data simultaneously.

A deep knowledge of the underlying architectural details of these servers is crucial to develop code that adequately uses the available, highly parallel, computing resources.
These challenges are even more difficult to overcome by non-computer scientists, which are responsible for the development of a significant amount of the applications that use large computing platforms.

The pipelined data analysis application is one of the most common types of these applications, which typically analyses large sets of experimental data to monitor, test, and/or prove hypothesis and theories.
Most pipelined data analyses apply a set of tasks, organised in a pipeline, on independent datasets.
These tasks are commonly followed by the bynary evaluation of a criterion (i.e., \textit{true} or \textit{false}), which dictates if a given dataset element can be further processed through the remaining tasks of the pipeline or is discarded.
The performance of these applications, which is affected by the computational complexity of the pipeline, is crucial to ensure that the large amounts of data are processed in reasonable time.

The Highly Efficient Pipeline framework (HEP-Frame) is a C++ tool that was developed to improve the performance of these applications, scheduling the propositions of the pipeline to be processed simultaneously in the available resources, while respecting their inter-dependencies.
This framework allows the user to code a pipeline of propositions, while automatically handling input reading, parallelization, and output storage.
To achieve this, HEP-Frame generates the required code at compile time, while the repetitive tasks are automated through scripts.
HEP-Frame has a scheduler developed to efficiently distribute the specific workloads of pipelined data analyses, by representing the pipeline propositions in dependency graphs to expose parallelism, among the computing resources available from single-node compute servers to large scale platforms.

The improvement in the performance of pipelined data analyses through the use of HEP-Frame allowed scientists to develop more computationally complex propositions and more intricate pipeline organisations, as such is the case of conditional pipelined data analysis applications.
These applications implement pipelines with conditional propositions, which evaluate a \textit{n-ary} criterion that may result in the discarding of a given dataset element (such as regular propositions), but also has control of which is the next proposition to be executed.
For instance, two dataset elements that are processed and pass the criterion of a proposition may be, depending on its result, fed to different following propositions.
This means that a dataset element is only processed through a subset of the conditional propositions in the pipeline to be successfully analysed.
The specific characteristics of conditional pipeline data analyses make it unsuitable to be implemented using HEP-Frame, and similar frameworks, or parallelized by traditional list/task schedulers.





%\subsection{Contexto scientific data analysis com pipelines que podem ter multiplas saidas que liga com o proximo ponto}
%In some cases, an analysis may be carried out where more than one result can be a legitimate outcome for a specific validation. These different constraints can lead to different validations paths and can create possible outcome ramification. This type of task is commonly found in fields of science, such as particle analysis, where we can distinguish different types of particles depending on the values that specific tests obtain.
%With the linking of tasks, we create a graph that describes a pipeline with a conditional flow.


%\subsection{Definir mais em detalhe o que sao pipelines que podem ter multiplas saidas, que liga com o proximo ponto} 
%This type of analysis can be described as a node connected to multiple exit nodes where we have one node for each possible successful outcomes and one to catch a possible fail case.
%In each of these nodes, we can expect to have a computation operation and a condition for each possible outcome and respective node.
%This can be thought out and modelled as a switch statement where a filter is applied and depending on the result raised by the condition a path is chosen for the next node. 

%\subsection{Definir Mais em detalhe o que sao pipelines condicionais}
%With the pipelines as mentioned earlier, we can describe the concept of conditional pipelines wherein each stage we can have multiple possible steps in which the next task to follow is calculated through a condition.

%\subsection{Intro to HEP-Frame}
% Typically these applications are structured as a sequence of tasks in a pipeline of actions, where data can be modified at each pipeline stage, filtered out and/or output as a result. The next tasks in the pipeline do not process the elements that are filtered out. Actions often vary from intensive computing tasks to simple evaluations that may discard irrelevant data.
%\par To aid the development of the software necessary for this kind of problems, a framework was created \textit{\textbf{HEP-Frame}}. This framework allows the user to create a pipeline of tasks and focusing on the result instead of writing the same boilerplate code. To achieve this, most of the duplicated code is generated at compile time through processors, and the repetitive tasks are automated through scripts. Alongside this with the intentions to make these calculations in the shortest time possible and the most efficient way possible. A special scheduler based on graph and list/queue schedulers was developed for this that allows efficient execution under multi-node computer clustering architectures, using parallel computing techniques.
%This framework allows for a particular type of process that is incredibly useful for pipeline streaming that if at any stage a validation fails then the data-set is discarded and we can start working on the next set.
%In the current state of the framework it does not support conditional pipelining.


%\subsection{Referir que ainda existe muito pouco trabalho na area de escalonadores para estas pipelines}
%Although conditional pipelines have been applied to different domains, at the time being, there is little work involving directly schedulers.